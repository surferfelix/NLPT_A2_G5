{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c231ac1b-f18a-4001-b0e7-56db1e8a913b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import sys\n",
    "from nltk import pos_tag, WordNetLemmatizer\n",
    "from typing import List\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import stanza\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8527d13-71b1-453d-97b9-44d9214fd0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Al', '-', 'Zaman', ':', 'American', 'forces', 'killed', 'Shaikh', 'Abdullah', 'al', '-', 'Ani', ',', 'the', 'preacher', 'at', 'the', 'mosque', 'in', 'the', 'town', 'of', 'Qaim', ',', 'near', 'the', 'Syrian', 'border', '.']\n"
     ]
    }
   ],
   "source": [
    "def extract_tokenized_sentences(file_path: str) -> List[List[str]]:\n",
    "    \"\"\"Extract a list of tokens for every sentence from the corpus and return it.\"\"\"\n",
    "    tokenized_sentences = []\n",
    "    sentence_tokens = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf8') as infile:\n",
    "        filereader = csv.reader(infile, delimiter='\\t', quotechar='\\\\')\n",
    "        for row in filereader:\n",
    "            if len(row)==1:\n",
    "                continue\n",
    "            if not row:  # empty line, end of sentence\n",
    "              \n",
    "                tokenized_sentences.append(sentence_tokens)\n",
    "                \n",
    "                sentence_tokens = []\n",
    "            else:\n",
    "                token = row[1]\n",
    "               \n",
    "                sentence_tokens.append(token)\n",
    "\n",
    "    tokenized_sentences.append(sentence_tokens)  # append the last sentence; there is no empty line at the end of file\n",
    "\n",
    "    return tokenized_sentences\n",
    "\n",
    "file = \"data/en_ewt-up-train.conllu\"\n",
    "data= extract_tokenized_sentences(file)\n",
    "for d in data:\n",
    "    print(d)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48e13f10-722e-429d-af61-b1091549baca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', 'Al', 'Al', 'PROPN', 'NNP', 'Number=Sing', '0', 'root', '0:root', 'SpaceAfter=No', '_']\n"
     ]
    }
   ],
   "source": [
    "file_path = \"data/en_ewt-up-train.conllu\"\n",
    "with open(file_path, 'r', encoding='utf8') as infile:\n",
    "    filereader = csv.reader(infile, delimiter='\\t', quotechar='\\\\')\n",
    "    for row in filereader:\n",
    "        if len(row)==1:\n",
    "            continue\n",
    "        if not row:  # empty line, end of sentence\n",
    "\n",
    "            # tokenized_sentences.append(sentence_tokens)\n",
    "\n",
    "            sentence_tokens = []\n",
    "        else:\n",
    "            predicate = row[:11]\n",
    "            print(predicate)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f6a6028-128d-4e24-bdfc-1ead866bbbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing en_ewt-up-train.conllu\n",
      "Preprocessing en_ewt-up-dev.conllu\n",
      "Preprocessing en_ewt-up-test.conllu\n"
     ]
    }
   ],
   "source": [
    "def generate_preprocessed_file(infile_path: str, outfile_path: str) -> None:\n",
    "    \"\"\"Generate a new file containing information obtained through preprocessing.\"\"\"\n",
    "    with open(outfile_path, 'w',  newline='', encoding='utf8') as outfile:\n",
    "        filewriter = csv.writer(outfile, delimiter='\\t', quotechar='\\\\')\n",
    "\n",
    "        with open(infile_path, 'r', encoding='utf8') as infile:\n",
    "            filereader = csv.reader(infile, delimiter='\\t', quotechar='\\\\')\n",
    "            \n",
    "\n",
    "            sentence_index = 0\n",
    "            token_index = 0\n",
    "            for inrow in filereader:\n",
    "                if len(inrow)==1:\n",
    "                    continue\n",
    "                if not inrow:  # empty line\n",
    "                    filewriter.writerow(inrow)  # write an empty row to output file\n",
    "                    \n",
    "                else:\n",
    "                    if token_index ==0:\n",
    "                        filewriter.writerow(['id','token','lemma','pos_category','pos_tag', \n",
    "                                             'passive/active','head','dep','rel','after','predicate'])\n",
    "                        predicate = inrow[10]\n",
    "                        if predicate != \"_\":\n",
    "                            predicate == \"predicate\"\n",
    "                            \n",
    "                        filewriter.writerow(inrow[:10]+[predicate])\n",
    "                        token_index += 1\n",
    "                    else:\n",
    "                        if predicate != \"_\":\n",
    "                            predicate == \"predicate\"\n",
    "                        filewriter.writerow(inrow[:10]+[predicate])\n",
    "                        \n",
    "\n",
    "\n",
    "paths = [\"data/en_ewt-up-train.conllu\", \"data/en_ewt-up-dev.conllu\", \"data/en_ewt-up-test.conllu\"]\n",
    "\n",
    "for path in paths:\n",
    "    print(f'Preprocessing {os.path.basename(path)}')\n",
    "\n",
    "    preprocessed_path = path.replace('.conllu', 'preprocessed.conllu')\n",
    "    generate_preprocessed_file(path, preprocessed_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4e0eb36d-9947-432a-bc25-576a599fa083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>token</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos_category</th>\n",
       "      <th>pos_tag</th>\n",
       "      <th>passive/active</th>\n",
       "      <th>head</th>\n",
       "      <th>dep</th>\n",
       "      <th>rel</th>\n",
       "      <th>after</th>\n",
       "      <th>predicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Al</td>\n",
       "      <td>Al</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Number=Sing</td>\n",
       "      <td>0</td>\n",
       "      <td>root</td>\n",
       "      <td>0:root</td>\n",
       "      <td>SpaceAfter=No</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>HYPH</td>\n",
       "      <td>_</td>\n",
       "      <td>1</td>\n",
       "      <td>punct</td>\n",
       "      <td>1:punct</td>\n",
       "      <td>SpaceAfter=No</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Zaman</td>\n",
       "      <td>Zaman</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Number=Sing</td>\n",
       "      <td>1</td>\n",
       "      <td>flat</td>\n",
       "      <td>1:flat</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>:</td>\n",
       "      <td>_</td>\n",
       "      <td>1</td>\n",
       "      <td>punct</td>\n",
       "      <td>1:punct</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>American</td>\n",
       "      <td>american</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>JJ</td>\n",
       "      <td>Degree=Pos</td>\n",
       "      <td>6</td>\n",
       "      <td>amod</td>\n",
       "      <td>6:amod</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217147</th>\n",
       "      <td>23</td>\n",
       "      <td>my</td>\n",
       "      <td>my</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>Number=Sing|Person=1|Poss=Yes|PronType=Prs</td>\n",
       "      <td>24</td>\n",
       "      <td>nmod:poss</td>\n",
       "      <td>24:nmod:poss</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217148</th>\n",
       "      <td>24</td>\n",
       "      <td>car</td>\n",
       "      <td>car</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>Number=Sing</td>\n",
       "      <td>21</td>\n",
       "      <td>obl</td>\n",
       "      <td>21:obl:on</td>\n",
       "      <td>SpaceAfter=No</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217149</th>\n",
       "      <td>25</td>\n",
       "      <td>)</td>\n",
       "      <td>)</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>-RRB-</td>\n",
       "      <td>_</td>\n",
       "      <td>4</td>\n",
       "      <td>punct</td>\n",
       "      <td>4:punct</td>\n",
       "      <td>SpaceAfter=No</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217150</th>\n",
       "      <td>26</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>.</td>\n",
       "      <td>_</td>\n",
       "      <td>4</td>\n",
       "      <td>punct</td>\n",
       "      <td>4:punct</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217151</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>217152 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id     token     lemma pos_category pos_tag  \\\n",
       "0        1        Al        Al        PROPN     NNP   \n",
       "1        2         -         -        PUNCT    HYPH   \n",
       "2        3     Zaman     Zaman        PROPN     NNP   \n",
       "3        4         :         :        PUNCT       :   \n",
       "4        5  American  american          ADJ      JJ   \n",
       "...     ..       ...       ...          ...     ...   \n",
       "217147  23        my        my         PRON    PRP$   \n",
       "217148  24       car       car         NOUN      NN   \n",
       "217149  25         )         )        PUNCT   -RRB-   \n",
       "217150  26         .         .        PUNCT       .   \n",
       "217151                                                \n",
       "\n",
       "                                    passive/active head        dep  \\\n",
       "0                                      Number=Sing    0       root   \n",
       "1                                                _    1      punct   \n",
       "2                                      Number=Sing    1       flat   \n",
       "3                                                _    1      punct   \n",
       "4                                       Degree=Pos    6       amod   \n",
       "...                                            ...  ...        ...   \n",
       "217147  Number=Sing|Person=1|Poss=Yes|PronType=Prs   24  nmod:poss   \n",
       "217148                                 Number=Sing   21        obl   \n",
       "217149                                           _    4      punct   \n",
       "217150                                           _    4      punct   \n",
       "217151                                                               \n",
       "\n",
       "                 rel          after predicate  \n",
       "0             0:root  SpaceAfter=No         _  \n",
       "1            1:punct  SpaceAfter=No         _  \n",
       "2             1:flat              _         _  \n",
       "3            1:punct              _         _  \n",
       "4             6:amod              _         _  \n",
       "...              ...            ...       ...  \n",
       "217147  24:nmod:poss              _         _  \n",
       "217148     21:obl:on  SpaceAfter=No         _  \n",
       "217149       4:punct  SpaceAfter=No         _  \n",
       "217150       4:punct              _         _  \n",
       "217151                                         \n",
       "\n",
       "[217152 rows x 11 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file = \"data/en_ewt-up-trainpreprocessed.conllu\"\n",
    "input_data = pd.read_csv(input_file, encoding='utf-8', sep='\\t', keep_default_na=False,     \n",
    "                             quotechar='\\\\', skip_blank_lines=False)\n",
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e06e51-f28d-435c-a946-be35c632813e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in terminal, if you don't have the package: python3 -m spacy download en_core_web_sm\n",
    "\n",
    "# def read_parse_result(parse_path: str):\n",
    "#     return pd.read_csv(parse_path, sep=\"\\t\", quotechar=\"|\", encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "# def prepare_data(content):\n",
    "#     # with open(text_path, encoding=\"utf-8\") as f:\n",
    "#     #     content = f.read()\n",
    "\n",
    "#     nlp = spacy.load(\"en_core_web_sm\")\n",
    "#     doc = nlp(content)\n",
    "\n",
    "#     # https://spacy.io/api/token\n",
    "#     token = [tok.text for tok in doc]\n",
    "#     dependency = [tok.dep_ for tok in doc]\n",
    "#     head = [tok.head for tok in doc]\n",
    "#     dependent = [[t.text for t in tok.children] for tok in doc]\n",
    "#     constituent = [[t.text for t in tok.subtree] for tok in doc]\n",
    "\n",
    "#     parse_info = {\"token\": token, \"dependency\": dependency,\n",
    "#                   \"head\": head, \"dependent\": dependent,\n",
    "#                   \"constituent\": constituent}\n",
    "\n",
    "#     df = pd.DataFrame.from_dict(parse_info)\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# def extract_head(dataframe) -> List[str]:\n",
    "#     return dataframe[\"head\"].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d824625f-b518-4c12-82e8-76059dfae114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bb4dcb-2580-480e-a849-80ca849281b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta_df= []\n",
    "# for i in range(len(data)):\n",
    "#     cols = [\n",
    "#         \"token\", \"dependency\", \"head\", 'dependent', \"consituent\"\n",
    "#     ]\n",
    "#     datas = ' '.join(data[i])\n",
    "#     datas = prepare_data(datas)\n",
    "#     meta = pd.DataFrame(datas)\n",
    "#     meta.columns = cols[0:]\n",
    "#     meta_df.append(meta)\n",
    "# print(meta_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ccba84-ec97-4188-8091-d6d1e6de5ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspired by https://github.com/cltl/ma-ml4nlp-labs/blob/main/code/assignment3/CRF.py, extract_sents_from_conll\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
