{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2a58950-dbfa-4062-a5f8-97d86550a46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import sys\n",
    "import pandas as pd\n",
    "from nltk.corpus import gutenberg\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dd4326f-3b37-4275-975e-5d657b3898b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"data/en_ewt-up-train.conllu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5c27527-6fa3-4286-9851-d03ac9a3326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pos_category(pos_tags: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Assign part-of-speech tags to seven categories as follows: ADJ, NN, ADV, VB, PRO, PUNCT and OTH.\n",
    "    :param pos_tags: list with pos_tags\n",
    "    :return: list with POS tags categories\n",
    "    \"\"\"\n",
    "    pos_list = []\n",
    "    adj = ['JJ', 'JJR', 'JJS']\n",
    "    nn = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "    adv = ['RB', 'RBR', 'RBS']\n",
    "    vb = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "    pro = ['PRP', 'PRP$']\n",
    "   \n",
    "    for pos_tag in pos_tags:\n",
    "        if not pos_tag:  # if we have an empty row\n",
    "            pos_list.append('')\n",
    "        elif pos_tag in adj:\n",
    "            pos_list.append('ADJ')\n",
    "        elif pos_tag in nn:\n",
    "            pos_list.append('NN')  \n",
    "        elif pos_tag in adv:\n",
    "            pos_list.append('ADV')  \n",
    "        elif pos_tag in pro:\n",
    "            pos_list.append('PRO')  \n",
    "        elif pos_tag in vb:\n",
    "            pos_list.append('VERB')  \n",
    "        elif pos_tag[0] in string.punctuation:\n",
    "            pos_list.append('PUNCT') \n",
    "        else:\n",
    "            pos_list.append('OTH')  \n",
    "        \n",
    "    return pos_list\n",
    "\n",
    "\n",
    "def extract_previous_and_next(elements: List[str]) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Extract previous and preceding token or lemma from a list of tokens or lemmas\n",
    "    :param elements: list with tokens or lemmas\n",
    "    :return: list with previous tokens/lemmas, list with next tokens/lemmas\n",
    "    \"\"\"\n",
    "    position_index = 0\n",
    "\n",
    "    prev_tokens = []\n",
    "    next_tokens = []\n",
    "\n",
    "    bos_previous, eos_next = False, False  # flags to tell us where sentences end\n",
    "\n",
    "    for i in range(len(elements)):\n",
    "\n",
    "        prev_index = (position_index - 1)\n",
    "        next_index = (position_index + 1)\n",
    "        \n",
    "        # previous token\n",
    "        if prev_index < 0:\n",
    "            previous_token = \"bos\"\n",
    "\n",
    "        else:\n",
    "            previous_token = elements[prev_index]\n",
    "            if previous_token == '':  # if there's an empty line before this token\n",
    "                previous_token = \"bos\"\n",
    "                bos_previous = True\n",
    "\n",
    "            if bos_previous:\n",
    "                prev_tokens[position_index - 1] = ''\n",
    "                bos_previous = False\n",
    "\n",
    "        prev_tokens.append(previous_token)\n",
    "            \n",
    "        # next token\n",
    "        if eos_next:\n",
    "            next_token = ''\n",
    "            eos_next = False\n",
    "\n",
    "        elif next_index < len(elements):\n",
    "            next_token = elements[next_index]\n",
    "            if next_token == '':  # if there's an empty line after this token\n",
    "                next_token = 'eos'\n",
    "                eos_next = True\n",
    "        else: \n",
    "            next_token = \"eos\"\n",
    "\n",
    "        next_tokens.append(next_token)\n",
    "            \n",
    "        # moving to next token in list\n",
    "        position_index += 1\n",
    "    \n",
    "    return prev_tokens, next_tokens\n",
    "        \n",
    "                            \n",
    "def get_negation_features(lemmas: list, single_neg_cues: set, neg_prefix: set, neg_suffix: set, vocab: set) \\\n",
    "        -> Tuple[List[str], List[str], List[str], List[str], List[str]]:\n",
    "    \"\"\"Extract negation features for each lemma, i.e. is_neg_cue, has_affix, affix, base_is_word and base.\"\"\"\n",
    "    neg_cues = []           \n",
    "    has_affix = []\n",
    "    affix = []\n",
    "    base_is_word = []\n",
    "    base = []\n",
    "\n",
    "    for lemma in lemmas:\n",
    "\n",
    "        if not lemma:  # empty line\n",
    "            is_neg_cue, has_affix_val, affix_val, base_is_word_val, base_val = '', '', '', '', ''\n",
    "         \n",
    "        # Assign values for the case that the token doesn't have the affixes\n",
    "        else:\n",
    "            is_neg_cue = 0\n",
    "            has_affix_val = 0\n",
    "            affix_val = \"\"\n",
    "            base_is_word_val = 0\n",
    "            base_val = \"\"\n",
    "            \n",
    "            if lemma in single_neg_cues:  # checking if lemma is found in list of collected single-word negation cues\n",
    "                is_neg_cue = 1\n",
    "                \n",
    "            if not is_neg_cue:  # if not, we continue checking for affixal values\n",
    "\n",
    "                # Check if the base does have the affixes\n",
    "                for suffix in neg_suffix:\n",
    "                    # If the token ends with one of the suffixes\n",
    "                    if lemma.endswith(suffix):\n",
    "                        # Get the base\n",
    "                        base_lemma = lemma.replace(suffix, \"\")\n",
    "                        if len(base_lemma) > 2:\n",
    "                            # has_affix has value 1\n",
    "                            has_affix_val = 1\n",
    "                            # Feature 'affix' captures the prefix\n",
    "                            affix_val = suffix\n",
    "                            # If the base is also in our vocab set\n",
    "                            if base_lemma in vocab:\n",
    "                                # base_is_word has value 1\n",
    "                                base_is_word_val = 1\n",
    "                                # Feature 'base' captures the base\n",
    "                                base_val = base_lemma\n",
    "                            break\n",
    "\n",
    "                if not has_affix_val:  # if lemma doesn't have a suffix\n",
    "                    # Check if it starts with one of the prefixes\n",
    "                    for prefix in neg_prefix:\n",
    "                        # If the lemma starts with one of the prefixes\n",
    "                        if lemma.startswith(prefix):\n",
    "                            # Get the base\n",
    "                            base_lemma = lemma.replace(prefix, \"\", 1)\n",
    "                            if len(base_lemma) > 3:\n",
    "                                # has_affix has value 1\n",
    "                                has_affix_val = 1\n",
    "                                # Feature 'affix' captures the prefix\n",
    "                                affix_val = prefix\n",
    "                                # If the base is also in our vocab set\n",
    "                                if base_lemma in vocab:\n",
    "                                    # base_is_word has value 1\n",
    "                                    base_is_word_val = 1\n",
    "                                    # Feature 'base' captures the base\n",
    "                                    base_val = base_lemma\n",
    "                                break\n",
    "\n",
    "        # Appending the values to the lists\n",
    "        neg_cues.append(is_neg_cue)\n",
    "        has_affix.append(has_affix_val)\n",
    "        affix.append(affix_val)\n",
    "        base_is_word.append(base_is_word_val)\n",
    "        base.append(base_val)\n",
    "\n",
    "    return neg_cues, has_affix, affix, base_is_word, base\n",
    "\n",
    "\n",
    "def write_features(input_file: str, output_file: str, neg_cues_set: set, neg_prefix: set, neg_suffix: set,\n",
    "                   vocab: set) -> None:\n",
    "    \"\"\"Generate a new file containing extracted features.\"\"\"\n",
    "\n",
    "    # Read in preprocessed file\n",
    "    input_data = pd.read_csv(input_file, encoding='utf-8', sep='\\t', header=None, keep_default_na=False,     \n",
    "                             quotechar='\\\\', skip_blank_lines=False)\n",
    "\n",
    "    # Extracting features and labels\n",
    "    tokens = input_data[3].astype('str').apply(lambda x: x.lower())\n",
    "    lemmas = input_data[4].astype('str').apply(lambda x: x.lower())\n",
    "    pos_tags = input_data[5]\n",
    "    labels = input_data[6]\n",
    "\n",
    "    # Extracting additional features\n",
    "    # prev_tokens, next_tokens = extract_previous_and_next(tokens)\n",
    "    prev_lemmas, next_lemmas = extract_previous_and_next(lemmas)\n",
    "    pos_categories = generate_pos_category(pos_tags)  \n",
    "\n",
    "    neg_cues, has_affix, affix, base_is_word, base = get_negation_features(lemmas, neg_cues_set, neg_prefix, neg_suffix,\n",
    "                                                                           vocab)\n",
    "\n",
    "    # Defining feature values for writing to output file\n",
    "    features_dict = {'book': input_data[0], 'sent_num': input_data[1], 'token_num': input_data[2],\n",
    "                     'token': tokens,  # 'prev_token': prev_tokens, 'next_token': next_tokens,\n",
    "                     'lemma': lemmas, 'prev_lemma': prev_lemmas, 'next_lemma': next_lemmas,\n",
    "                     # 'pos_tag': pos_tags,\n",
    "                     'pos_category': pos_categories,\n",
    "                     'is_single_cue': neg_cues,\n",
    "                     'has_affix': has_affix, 'affix': affix,\n",
    "                     'base_is_word': base_is_word, 'base': base,\n",
    "                     'gold_label': labels}\n",
    "\n",
    "    # Defining header names\n",
    "    feature_names = features_dict.keys()\n",
    "\n",
    "    features_df = pd.DataFrame(features_dict, columns=feature_names)\n",
    "    # Remove empty rows\n",
    "    features_df_clean = features_df[features_df['token'] != '']\n",
    "\n",
    "    # Writing features to file\n",
    "    features_df_clean.to_csv(output_file, sep='\\t', index=False)\n",
    "\n",
    "\n",
    "def main(paths=None) -> None:\n",
    "    \"\"\"Extend the features for preprocessed file and save a features-added version.\"\"\"\n",
    "    if not paths:  # if no paths are passed to the function\n",
    "        paths = sys.argv[1:]\n",
    "\n",
    "    if not paths:  # if no paths are passed to the function through the command line\n",
    "        paths = [CONFIG['train_path'].replace('.txt', '_preprocessed.txt'),\n",
    "                 CONFIG['dev_path'].replace('.txt', '_preprocessed.txt')]\n",
    "        \n",
    "    single_neg_cues_file = CONFIG['single_neg_cues_file']\n",
    "    neg_cues_set = set()\n",
    "        \n",
    "    with open(single_neg_cues_file, 'r', encoding='utf8') as infile:\n",
    "        for line in infile:\n",
    "            neg_cues_set.add(line.strip())    \n",
    "\n",
    "    # Create negation prefix set and suffix set\n",
    "    # Negation affixes are acquired from the training and dev data sets\n",
    "    # Common negation prefix \"non-\" is manually added\n",
    "    neg_prefix = {\"dis\", \"im\", \"in\", \"ir\", \"un\", \"non\"}\n",
    "    neg_suffix = {\"less\", \"lessness\", \"lessly\"}\n",
    "\n",
    "    # Create corpus vocab set from the Gutenberg corpus in NLTK\n",
    "    print(\"Building vocab set...\")\n",
    "    vocab = {word.lower() for word in gutenberg.words()}\n",
    "    print(\"Done\")\n",
    "\n",
    "    for path in paths:\n",
    "        # Prepare output file\n",
    "        output_file = path.replace('_preprocessed.txt', '_features.txt')\n",
    "        print(f'Extracting features from {os.path.basename(path)}')\n",
    "        write_features(path, output_file, neg_cues_set, neg_prefix, neg_suffix, vocab)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dca580-0a19-40ea-a2d1-74a3dc9f7527",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
